# State-of-the-art Seminar

## **Title** : Deep Generative Modeling

### **_By_** : Gyan Prabhat
### **Supervisor**: [Dr. Anand Mishra](https://anandmishra22.github.io/) <br>

#### Overview
We are currently in the era of Generative Artificial Intelligence (AI). Departing from the conventional approach of solely identifying patterns within data, we have taken a step forward with a paradigm-shifting idea of generating brand new data instances by leveraging the patterns we have learnt from the given data. This novel direction can be categorized as Deep Generative Modeling, which constitutes a specialized branch of Deep Learning. The ultimate objective of Deep Generative Modeling is to focus on creating sophisticated models capable of generating new data that is indistinguishable from real examples. These methodologies typically fall under the category of unsupervised learning. This approach goes beyond mere data pattern recognition; it endeavours to understand the underlying data distribution and generate entirely novel instances that conform to that distribution. Deep Generative Models, like Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), have shown remarkable success in diverse applications, including image synthesis, text generation, handwritten text generation and data augmentation, making them a pivotal part of contemporary AI research.

### **Resources**
- Presentation - [Link](https://drive.google.com/file/d/1rHL9csC44jBdCbTYo6JD4TXGZ_AW-Sfj/view?usp=sharing)

### Tutorial Plan
1. **Introduction**
   * Machine Learning Paradigm
     - Supervised vs Unsupervised Models
   * Discriminative Models
   * Generative Models
   * Why Generative Models?
       - Debiasing
       - Outlier detection
2. **Autoencoders**
   * Introduction
   * Latent Space
   * Architecture
     - Loss Function
   * Applications
     - Feature Learning
     - Dimensionality Reduction
   * Takeaways
3. **Variational Autoencoders**
   * Introduction
   * Traditional vs Variational Autoencoders
   * Architecture
     - Encoder
     - Decoder
   * Loss Function
     - Reconstruction Loss
     - Regularization Loss
4. **Generative adversarial networks (GAN)**
   * Introduction
     - Issues
     - Ideas
   * Architecture
     - Generator Network
     - Discriminator Network
   * Training
     - Min-Max Game
     - Gradient Updates
5. **Diffusion Models**
   * Introduction
     - What is Diffusion?
     - Diffusion in AI
     - Trailer of Diffusion model
     - Intuition of Diffusion model
   * Architecture
     - Diffusion Process
       - Forward Noising
       - Reverse Denoising
     - Sampling brand new Generations
6. **Evaluation of Generative Models**
   * Know the Ground Truth
     - MSE (Mean Squared Error)
     - PSNR (Peak Signal to Noise Ratio)
     - SSIM (Structural Similarity)
   * Unknown Ground Truth
     - IS (Inception Score)
     - FID (Fr√©chet Inception Distance)
     - KID (Kernel Inception Distance)
   * Human Evaluation

### Reference Papers
1. Bank et al., [Autoencoders](https://arxiv.org/pdf/2003.05991.pdf)
2. Kingma et al., [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114v10.pdf)
3. Sohl-Dickstein et al., [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/pdf/1503.03585.pdf)
4. Song et al., [Generative Modeling by Estimating Gradients of the Data Distribution](https://arxiv.org/pdf/1907.05600.pdf)
5. Ho et al., [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf) [Github](https://github.com/hojonathanho/diffusion)
6. Rumelhart et al., [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)
7. Kingma et al., [Semi-Supervised Learning with Deep Generative Models](https://arxiv.org/pdf/1406.5298.pdf)
8. Doersch et al., [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf)
9. Kingma et al., [An Introduction to Variational Autoencoders](https://arxiv.org/pdf/1906.02691.pdf)
10. Kingma et al., [Variational Autoencoders for Collaborative Filtering](https://arxiv.org/pdf/1802.05814.pdf)
11. Goodfellow et al., [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661.pdf)
12. Radford et al., [Unsupervised representation learning with deep convolutional generative adversarial networks](https://arxiv.org/pdf/1511.06434.pdf)
13. Gulrajani et al., [Improved Training of Wasserstein GANs](https://arxiv.org/pdf/1704.00028.pdf)
14. Karras et al., [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/pdf/1710.10196.pdf)
15. Brock et al., [BigGAN: Large-Scale Generative Adversarial Networks](https://arxiv.org/pdf/1809.11096.pdf)
16. Dhariwal et al., [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/pdf/2105.05233.pdf)
17. Fogel et al., [ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation](https://arxiv.org/pdf/2003.10557v1.pdf) [Github](https://github.com/amzn/convolutional-handwriting-gan)
18. Luhman et al., [Diffusion models for Handwriting Generation](https://arxiv.org/pdf/2011.06704.pdf) [Github](https://github.com/tcl9876/Diffusion-Handwriting-Generation)
19. Bhunia et al., [Handwriting Transformers](https://arxiv.org/pdf/2104.03964.pdf) [Github](https://github.com/ankanbhunia/Handwriting-Transformers)
20. Gan et al., [HiGAN: Handwriting Imitation Conditioned on Arbitrary-Length Texts and Disentangled Styles](https://ojs.aaai.org/index.php/AAAI/article/view/16917/16724)
21. Pippi et al., [Handwritten Text Generation from Visual Archetypes](https://arxiv.org/pdf/2303.15269.pdf)

<br>
[Contact : Gyan Prabhat](prabhat.1@iitj.ac.in)


